# Linear-Regression-Regularization
# Regularization in Linear Regression

This project demonstrates the implementation and comparison of different **regularization techniques** for linear regression. It explores how **L1 (Lasso)**, **L2 (Ridge)**, and **Elastic Net** regularization methods improve model performance, especially when working with **noisy data** and **outliers**.

---

## **Objectives**

After completing this project, you will be able to:

- Implement, evaluate, and compare the performance of three regularization techniques for linear regression.
- Analyze the effect of simple linear regularization when modeling on noisy datasets with and without outliers.
- Use **Lasso regularization** to reduce the number of features for subsequent multiple linear regression modeling.

---

## **Project Overview**

Regularization is a crucial technique in machine learning to **prevent overfitting** and improve **generalization**.  
In this project, we:

- Built a baseline linear regression model.
- Applied **Ridge**, **Lasso**, and **Elastic Net** regularization.
- Compared their performance using metrics like **RÂ² score**, **Mean Squared Error (MSE)**, and **Mean Absolute Error (MAE)**.
- Visualized the impact of regularization on coefficient shrinkage.
- Demonstrated how Lasso can be used for **feature selection**.

---

## **Technologies Used**

- **Python 3.x**
- **NumPy**
- **Pandas**
- **Scikit-learn**
- **Matplotlib**
- **Seaborn**
- **Jupyter Notebook**

---
